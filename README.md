# AI 351 – Natural Language Processing

This repository contains a series of assignments and experiments completed as part of the **AI 351: Natural Language Processing** course. Each module demonstrates a foundational concept or model in NLP, including tokenization, n-gram models, named entity recognition (NER), word embeddings, recurrent neural networks (RNNs), and Transformer-based architectures.

---

## Necessary Files

| File / Notebook                        | Description |
|----------------------------------------|-------------|
| `MEx2_Tokenizer.ipynb`                | Custom tokenizer implementation and exploration of tokenization techniques. |
| `MEx3_N_grams.ipynb`                  | Implementation of n-gram language models with probability estimates and smoothing. |
| `MEx4_NER_with_CRF.ipynb`             | Named Entity Recognition using Conditional Random Fields (CRFs). |
| `MEx4_ner_json.txt`<br/>`MEx4_test_ner_json.txt` | Input/output data used in the NER-CRF model. |
| `MEx5_Fasttext_Embedding.py`          | FastText embedding generation script using Gensim or Facebook's FastText. |
| `MEx5_Fasttext_Embedding_Results.pdf` | Sample results and performance metrics of the FastText embeddings. |
| `MEx6_RNN_with_LSTM.ipynb`            | Sequence modeling with RNNs and LSTM architectures for text classification. |
| `MEx6_ner_json.txt`<br/>`MEx6_test_ner_json.txt` | JSON-formatted dataset for sequence labeling or NER task. |
| `MEx7_Transformers.ipynb`             | Transformer-based NLP model implementation (e.g., using HuggingFace or PyTorch). |


---

## Author

**Jeryl Salas**  
Master of Engineering in Artificial Intelligence – University of the Philippines Diliman  
GitHub: [@zencordia4913](https://github.com/zencordia4913)  
Weights & Biases Portfolio: [wandb.ai/jasalas](https://wandb.ai/jasalas)  
HuggingFace: [huggingface.co/Jeryl](https://huggingface.co/Jeryl)

