{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Exercise 3\n",
    "### By: Jeryl Salas\n",
    "Using a regex tokenizer, we obtain a trigram language model for that dataset with a simple interpolation to get a lambda score to get a probability score for unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jeryl\n",
      "[nltk_data]     Salas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np # For computations of probabilities, perplexities and expected count\n",
    "import os # For folder and file path opening\n",
    "import json # Used for loading JSON data\n",
    "import random # For random selection of JSON files for training and testing\n",
    "import pandas as pd # For transforming JSON data into a pandas dataframe\n",
    "import nltk # Used for tokenization of training and testing set\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Loading and Preprocessing Data\n",
    "\n",
    "We first load the dataset and perform tokenization. The training set would consist of 1000 JSON files while testing set would containt 10 JSON files as instructed by Sir Migz. Both training and testing set are tokenized in the preprocess function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def load_dataset(folder_path, n):\n",
    "    \"\"\"\n",
    "    Using the os and JSON libraries, data gets loaded into a pandas df with training and testing data separated\n",
    "    \"\"\"\n",
    "    files = [file for file in os.listdir(folder_path) if file.endswith('.json')]\n",
    "    selected_files = random.sample(files, n)\n",
    "    selected_files_test = selected_files[:10]\n",
    "    selected_files_train = selected_files[10:]\n",
    "    train_loaded_data = []\n",
    "    test_loaded_data = []\n",
    "\n",
    "    for file_name in selected_files_train:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file) # Load JSON data\n",
    "                train_loaded_data.append(data)\n",
    "                #print(f\"successfully loaded {file_name}.JSON\") # Print for checking\n",
    "        except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "            print(f\"error loading {file_name}: {e}\")\n",
    "\n",
    "    print(\"JSON files used for testing:\")\n",
    "    for file_name in selected_files_test:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file) # Load JSON data\n",
    "                test_loaded_data.append(data)\n",
    "                print(f\"successfully loaded {file_name}.JSON\") # Print for checking\n",
    "        except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "            print(f\"error loading {file_name}: {e}\")\n",
    "\n",
    "\n",
    "    pd.set_option('display.max_colwidth', 100)  \n",
    "    pd.set_option('display.width', 200)         \n",
    "    pd.set_option('display.max_rows', 10) \n",
    "\n",
    "    print(f\"Successffully loaded {len(train_loaded_data) + len(test_loaded_data)} JSON files.\")\n",
    "    train_flattened_data = [item for sublist in train_loaded_data for item in sublist] # Flatten data \n",
    "    train_df = pd.DataFrame(train_flattened_data) # Convert to pandas dataframe   \n",
    "\n",
    "    test_flattened_data = [item for sublist in test_loaded_data for item in sublist] # Flatten data \n",
    "    test_df = pd.DataFrame(test_flattened_data) # Convert to pandas dataframe   \n",
    "\n",
    "    return train_df['text'], test_df['text']\n",
    "\n",
    "\n",
    "def replace_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replaces specific special characters in the input text according to predefined replacement rules.\n",
    "    \"\"\"\n",
    "    replacement_rules = {'“': '\"', '”': '\"', '’': \"'\", '--': ','}\n",
    "    for symbol, replacement in replacement_rules.items():\n",
    "        text = text.replace(symbol, replacement)\n",
    "    return text\n",
    "\n",
    "def generate_tokens(paragraph: str) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Tokenize sentences using RegexpTokenizer and appending '[END]' token on each setence\n",
    "    \"\"\"\n",
    "    word_tokenizer = RegexpTokenizer(r'[-\\'\\w]+')\n",
    "\n",
    "    for sentence in sent_tokenize(paragraph):\n",
    "        tokenized_sentence = word_tokenizer.tokenize(sentence)\n",
    "        if tokenized_sentence:\n",
    "            tokenized_sentence.append('[END]')\n",
    "            yield tokenized_sentence\n",
    "\n",
    "def preprocess(folder_path):\n",
    "    \"\"\"\n",
    "    Main preprocessing function used to load and process text data and output tokens for both training and testing\n",
    "    \"\"\"\n",
    "    train_set, test_set = load_dataset(folder_path, 1010)\n",
    "\n",
    "    train_tokenized_sentences = []\n",
    "    test_tokenized_sentences = []\n",
    "\n",
    "    for text in train_set:\n",
    "        cleaned_text = replace_characters(text.lower())\n",
    "        for tokenized_sentence in generate_tokens(cleaned_text):\n",
    "            train_tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "    for text in test_set:\n",
    "        cleaned_text = replace_characters(text.lower())\n",
    "        for tokenized_sentence in generate_tokens(cleaned_text):\n",
    "            test_tokenized_sentences.append(tokenized_sentence)\n",
    "    \n",
    "    return train_tokenized_sentences, test_tokenized_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Generate N grams\n",
    "\n",
    "With this class, we can count the unigrams, bigrams, and trigrams. This is where they can also compute the respective n gram probabilities. These functions will be accessed in both the EM Algorithm and the generation of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class generate_Ngrams:\n",
    "    \"\"\"\n",
    "    Main class for dealing with N grams involving functions such as counting and calculating probabilities (unigram, bigram, trigram)\n",
    "    \"\"\"\n",
    "    def __init__(self, filename: str) -> None:\n",
    "        self.sentences, self.test_sentences = preprocess(filename)\n",
    "        self.unigram_counts = {}\n",
    "        self.total_unigrams = sum(self.unigram_counts.values())\n",
    "        self.bigram_counts = {}\n",
    "        self.trigram_counts = {}\n",
    "        self.unigram_prob = {}\n",
    "        self.bigram_prob = {}\n",
    "        self.trigram_prob = {}\n",
    "        self.a = 1\n",
    "        self.count()\n",
    "    \n",
    "    def count_ngrams(self, tokens, n):\n",
    "        \"\"\"\n",
    "        Used for countring bigrams and trigrams\n",
    "        \"\"\"\n",
    "        ngram_counts = {}\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i + n])\n",
    "            if ngram in ngram_counts:\n",
    "                ngram_counts[ngram] += 1\n",
    "            else:\n",
    "                ngram_counts[ngram] = 1\n",
    "        return ngram_counts\n",
    "    \n",
    "    def count(self):\n",
    "        \"\"\"\n",
    "        Main function for storing unigram, bigram, and trigram counts on a dict\n",
    "        \"\"\"\n",
    "        for tokens in self.sentences:\n",
    "            for unigram in tokens:\n",
    "                self.unigram_counts[unigram] = self.unigram_counts.get(unigram, 0) + 1  \n",
    "            \n",
    "            bigrams = self.count_ngrams(tokens, 2)\n",
    "            for bigram, count in bigrams.items():\n",
    "                if bigram in self.bigram_counts:\n",
    "                    self.bigram_counts[bigram] += count\n",
    "                else:\n",
    "                    self.bigram_counts[bigram] = count\n",
    "                 \n",
    "            trigrams = self.count_ngrams(tokens, 3)\n",
    "            for trigram, count in trigrams.items():\n",
    "                if trigram in self.trigram_counts:\n",
    "                    self.trigram_counts[trigram] += count\n",
    "                else:\n",
    "                    self.trigram_counts[trigram] = count\n",
    "\n",
    "    def calc_unigram_prob(self, word):\n",
    "        \"\"\"\n",
    "        Function for calculating unigram probability. Used Laplace smoothing with a = 1\n",
    "        \"\"\"\n",
    "        uni_count = self.unigram_counts.get(word, 0)\n",
    "        prob = (uni_count + self.a) / (self.total_unigrams + self.a * len(self.unigram_counts))\n",
    "        #print(f\"unigram prob = {prob}\")\n",
    "        return prob\n",
    "    \n",
    "    def calc_bigram_prob(self, w1, w2):\n",
    "        \"\"\"\n",
    "        Function for calculating bigram probability. Used Laplace smoothing with a = 1\n",
    "        \"\"\"\n",
    "        uni_count = self.unigram_counts.get(w1, 0)\n",
    "        bi_count = self.bigram_counts.get((w1, w2), 0)\n",
    "        vocab_size = len(self.unigram_counts)\n",
    "        prob = (bi_count + self.a) / (uni_count + self.a * vocab_size)\n",
    "        #print(f\"bigram prob = {prob}\")\n",
    "        return prob\n",
    "    \n",
    "    def calc_trigram_prob(self, w1, w2, w3):\n",
    "        \"\"\"\n",
    "        Function for calculating trigram probability. Used Laplace smoothing with a = 1\n",
    "        \"\"\"\n",
    "        bi_count = self.bigram_counts.get((w1, w2), 0)\n",
    "        tri_count = self.trigram_counts.get((w1, w2, w3), 0)\n",
    "        vocab_size = len(self.unigram_counts)\n",
    "\n",
    "        prob = (tri_count + self.a) / (bi_count + self.a * vocab_size)\n",
    "        #print(f\"trigram prob = {prob}\")\n",
    "        return prob\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perplexity Computation and Lambda Optimizer\n",
    "\n",
    "With this step, we compute interpolation probabilities for each token in training sentences and we use n gram probabilities as our basis in updating our lambdas via EM algorithm. The changes in perplexity will also be observed each iteration. For now, we use max iterations of 20 with stopping critera. The model also contains a generate_sentence function used to generate sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON files used for testing:\n",
      "successfully loaded d6cd8944-d555-455f-80ad-b3c7b4f91378.json.JSON\n",
      "successfully loaded c7f66770-5fb9-4e86-8d22-51a0aa2bbee9.json.JSON\n",
      "successfully loaded d7a567cf-31a1-4b6f-b1bf-c7eb7f0acea4.json.JSON\n",
      "successfully loaded 1fb0ce33-bbdf-4fcb-b110-5494cf84fc1f.json.JSON\n",
      "successfully loaded 57874f05-16b2-4951-bd04-9c8273c8f465.json.JSON\n",
      "successfully loaded 10baef64-43cf-477e-9b1b-8e0f4b568ca4.json.JSON\n",
      "successfully loaded b345ff09-64bf-4473-935f-fef4f9da6f23.json.JSON\n",
      "successfully loaded ebf6d539-b774-4828-9500-c93302fd4e67.json.JSON\n",
      "successfully loaded 47c625f2-e722-41b3-8373-9a07224f99a0.json.JSON\n",
      "successfully loaded c29968c7-7f30-400a-8725-0f62cbcb8061.json.JSON\n",
      "Successffully loaded 1010 JSON files.\n",
      "avg log prob = -23177273.045210753 / 7424014 = -3.121932831108717\n",
      "updt. perplexity = 22.6901935990979\n",
      "updt. lambdas = [0.0198029  0.03842729 0.94176982]\n",
      "Iteration 0, Perplexity: 22.6901935990979\n",
      "avg log prob = -23336055.638452355 / 7424014 = -3.143320532322859\n",
      "updt. perplexity = 23.180711507372443\n",
      "updt. lambdas = [7.26207118e-04 2.32876346e-03 9.96945029e-01]\n",
      "Iteration 1, Perplexity: 23.180711507372443\n",
      "avg log prob = -22966330.39911281 / 7424014 = -3.093519273955142\n",
      "updt. perplexity = 22.054557589920456\n",
      "updt. lambdas = [2.59146631e-05 1.36333709e-04 9.99837752e-01]\n",
      "Iteration 2, Perplexity: 22.054557589920456\n",
      "avg log prob = -22947544.564899642 / 7424014 = -3.0909888592477928\n",
      "updt. perplexity = 21.998820961163542\n",
      "updt. lambdas = [9.23467221e-07 7.96771596e-06 9.99991109e-01]\n",
      "Iteration 3, Perplexity: 21.998820961163542\n",
      "avg log prob = -22946552.000430238 / 7424014 = -3.0908551627772036\n",
      "updt. perplexity = 21.995879993046824\n",
      "updt. lambdas = [3.29052289e-08 4.65613500e-07 9.99999501e-01]\n",
      "Iteration 4, Perplexity: 21.995879993046824\n",
      "avg log prob = -22946497.759997208 / 7424014 = -3.09084785669817\n",
      "updt. perplexity = 21.99571928999623\n",
      "updt. lambdas = [1.17248302e-09 2.72091631e-08 9.99999972e-01]\n",
      "Iteration 5, Perplexity: 21.99571928999623\n",
      "avg log prob = -22946494.724283602 / 7424014 = -3.0908474477935526\n",
      "updt. perplexity = 21.99571029584689\n",
      "updt. lambdas = [4.17780443e-11 1.59002769e-09 9.99999998e-01]\n",
      "Iteration 6, Perplexity: 21.99571029584689\n",
      "avg log prob = -22946494.551523853 / 7424014 = -3.090847424523156\n",
      "updt. perplexity = 21.995709783997995\n",
      "updt. lambdas = [1.48863986e-12 9.29167872e-11 1.00000000e+00]\n",
      "Iteration 7, Perplexity: 21.995709783997995\n",
      "avg log prob = -22946494.542082213 / 7424014 = -3.0908474232513856\n",
      "updt. perplexity = 21.995709756024503\n",
      "updt. lambdas = [5.30433791e-14 5.42979810e-12 1.00000000e+00]\n",
      "Iteration 8, Perplexity: 21.995709756024503\n",
      "avg log prob = -22946494.5412612 / 7424014 = -3.0908474231407967\n",
      "updt. perplexity = 21.995709753592024\n",
      "updt. lambdas = [1.89004751e-15 3.17302269e-13 1.00000000e+00]\n",
      "Iteration 9, Perplexity: 21.995709753592024\n",
      "avg log prob = -22946494.54122001 / 7424014 = -3.0908474231352487\n",
      "updt. perplexity = 21.995709753469992\n",
      "updt. lambdas = [6.73463805e-17 1.85422603e-14 1.00000000e+00]\n",
      "Iteration 10, Perplexity: 21.995709753469992\n",
      "avg log prob = -22946494.54121945 / 7424014 = -3.0908474231351732\n",
      "updt. perplexity = 21.99570975346833\n",
      "updt. lambdas = [2.39969363e-18 1.08355802e-15 1.00000000e+00]\n",
      "Converged after 11 iterations.\n"
     ]
    }
   ],
   "source": [
    "class EM_Algorithm:\n",
    "    \"\"\"\n",
    "    Main class for the training model. Functions include interpolation, updating of expectations and lambdas, and the main optimization function that facilitates the EM Algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, gen_data, lambdas, eps):\n",
    "        self.data = gen_data\n",
    "        self.lambdas = lambdas\n",
    "        self.eps = eps\n",
    "        self.total_unigrams = sum(self.data.unigram_counts.values())\n",
    "\n",
    "    def compute_interpolated_probability(self, w1, w2, w3, lambdas):\n",
    "        \"\"\"\n",
    "        Computes interpolated probability which inclues the unigram, bigram, and trigram probabilities\n",
    "        \"\"\"\n",
    "        p_tri = self.data.calc_trigram_prob(w1, w2, w3)\n",
    "        p_bi = self.data.calc_bigram_prob(w2, w3)\n",
    "        p_uni = self.data.calc_unigram_prob(w3)\n",
    "        inter_p = lambdas[0] * p_tri + lambdas[1] * p_bi + lambdas[2] * p_uni\n",
    "        inter_p = max(inter_p, 1e-12)\n",
    "        #print(f\"inter prob = {inter_p}\")\n",
    "        log_inter_p = np.log(inter_p)\n",
    "        #print(f\"log inter prob = {log_inter_p}\")\n",
    "        return log_inter_p, inter_p, p_tri, p_bi, p_uni\n",
    "\n",
    "    def expectation_step(self, sentences, lambdas):\n",
    "        \"\"\"\n",
    "        Function used to update expectations of counts per sentence iteration and computes total log probability for perplexity computation\n",
    "        \"\"\"\n",
    "        total_log_prob = 0\n",
    "        expected_counts = np.zeros(3) \n",
    "        #print(f\"initial exp count = {expected_counts}\")\n",
    "\n",
    "        for tokens in sentences:\n",
    "            #print(f\"for sentence = {s}\")\n",
    "            #print(\"_____________________\")\n",
    "            for i in range(2, len(tokens)):\n",
    "                w1, w2, w3 = tokens[i-2], tokens[i-1], tokens[i]\n",
    "                #print(f\"for iter {i}: w1={w1}, w2={w2}, w3={w3}, lambdas={lambdas}, tokens={tokens}\")\n",
    "                log_inter_p, total_p, p_tri, p_bi, p_uni =  self.compute_interpolated_probability(w1, w2, w3, lambdas)\n",
    "                total_log_prob += log_inter_p\n",
    "                #print(f\"cumulative log prob = {total_log_prob}\")\n",
    "\n",
    "                if total_p > 0:\n",
    "                    expected_counts[0] += (lambdas[0] * p_tri) / total_p\n",
    "                    expected_counts[1] += (lambdas[1] * p_bi) / total_p\n",
    "                    expected_counts[2] += (lambdas[2] * p_uni) / total_p\n",
    "                #print(f\"updt. expected counts = {expected_counts}\")\n",
    "\n",
    "        #print(f\"after exp. step: total log prob = {total_log_prob}, exp counts = {expected_counts}\")\n",
    "        #print(\"___________________________________________________________________________________\")\n",
    "        return total_log_prob, expected_counts\n",
    "\n",
    "    def update_lambdas(self, exp_counts):\n",
    "        \"\"\"\n",
    "        Function used to update lambdas\n",
    "        \"\"\"\n",
    "        total_counts = np.sum(exp_counts)\n",
    "        new_lambdas = exp_counts / total_counts\n",
    "        print(f\"updt. lambdas = {new_lambdas}\")\n",
    "        return new_lambdas\n",
    "\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Main function that facilitates the update of lambdas and computation of perplexity\n",
    "        \"\"\"\n",
    "        prev_perplexity = float('inf')\n",
    "        for iteration in range(20):\n",
    "            total_log_prob, expected_counts = self.expectation_step(generated_data.sentences, self.lambdas)\n",
    "            average_log_prob = total_log_prob / self.total_unigrams\n",
    "            print(f\"avg log prob = {total_log_prob} / {self.total_unigrams} = {average_log_prob}\")\n",
    "            current_perplexity = np.exp(-average_log_prob)\n",
    "            print(f\"updt. perplexity = {current_perplexity}\")\n",
    "            self.lambdas = self.update_lambdas(expected_counts)\n",
    "\n",
    "            if abs(current_perplexity - prev_perplexity) < self.eps:\n",
    "                print(f\"Converged after {iteration} iterations.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Iteration {iteration}, Perplexity: {current_perplexity}\")\n",
    "            prev_perplexity = current_perplexity\n",
    "        \n",
    "\n",
    "        return self.lambdas, current_perplexity\n",
    "    \n",
    "    def generate_sentence(self, max_length=20):\n",
    "        \"\"\"\n",
    "        Function that generates sentences using the model's hyperparameters\n",
    "        \"\"\"\n",
    "        sentence = ['<s>', '<s>'] \n",
    "        while len(sentence) < max_length:\n",
    "            w1, w2 = sentence[-2], sentence[-1]\n",
    "            possible_words = list(self.data.unigram_counts.keys())\n",
    "            probabilities = []\n",
    "\n",
    "            for w3 in possible_words:\n",
    "                _, inter_p, _, _, _ = self.compute_interpolated_probability(w1, w2, w3, self.lambdas)\n",
    "                probabilities.append(inter_p)\n",
    "            \n",
    "            probabilities = np.array(probabilities) / sum(probabilities)\n",
    "\n",
    "            next_word = np.random.choice(possible_words, p=probabilities)\n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "            sentence.append(next_word)\n",
    "        \n",
    "        return ' '.join(sentence[2:])\n",
    "\n",
    "\n",
    "generated_data = generate_Ngrams(r'C:\\Users\\Jeryl Salas\\Documents\\AI 351\\MEx 2 Tokenizer\\coleridgeinitiative-show-us-the-data\\train')\n",
    "lambdas = np.array([0.9, 0.9, 0.9])\n",
    "eps = 1e-10\n",
    "model = EM_Algorithm(generated_data, lambdas, eps)\n",
    "lambdas, perp = model.optimize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Testing N-gram model\n",
    "We test the N gram model's performance with computation of average perplexity using the testing set consisting 10 JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized lambdas: [2.39969363e-18 1.08355802e-15 1.00000000e+00]\n",
      "Training Perplexity: 21.99570975346833\n",
      "Test Perplexity on Unseen Data: 1.0279437846621553\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, test_sentences):\n",
    "    \"\"\"\n",
    "    Function that calculates perplexity of the LM using the testing set\n",
    "    \"\"\"\n",
    "    total_perplexity = 0\n",
    "    num_sentences = len(test_sentences)\n",
    "\n",
    "    total_log_prob = 0\n",
    "    for tokens in test_sentences:\n",
    "        for i in range(2, len(tokens)):\n",
    "            w1, w2, w3 = tokens[i-2], tokens[i-1], tokens[i]\n",
    "            log_inter_p, _, _, _, _ =  model.compute_interpolated_probability(w1, w2, w3, lambdas)\n",
    "            total_log_prob += log_inter_p\n",
    "    \n",
    "    average_log_prob = total_log_prob / model.total_unigrams\n",
    "    return np.exp(-average_log_prob)\n",
    "\n",
    "# Printing of results\n",
    "avg_perp = calculate_perplexity(model, generated_data.test_sentences)\n",
    "print(f\"Optimized lambdas: {lambdas}\")  \n",
    "print(f\"Training Perplexity: {perp}\") \n",
    "print(f\"Test Perplexity on Unseen Data: {avg_perp}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Generate Sample Text\n",
    "We use generate sentence function inside the model in order to generate two sentences using the model's optimized lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence 1: is range size et than org mahoney estimate nodes confirmed grain scanner apoe adc-nlc became the lower such\n",
      "Generated Sentence 2: image statistics description 5 nonexplanatory in [END] [END] huxley each rotations acidification j low-income delaware national scenarios relative\n"
     ]
    }
   ],
   "source": [
    "# Generation of sentences\n",
    "sentence1 = model.generate_sentence()\n",
    "sentence2 = model.generate_sentence()\n",
    "\n",
    "print(\"Generated Sentence 1:\", sentence1)\n",
    "print(\"Generated Sentence 2:\", sentence2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Results\n",
    "We were able create a trigram model based on training data consisting of 1000 JSON files and test it's perplexity on testing data consisting of 10 JSON files that is completely seperate from the training set. We were able to get optimized lambdas ([2.39969363e-18 1.08355802e-15 1.00000000e+00]) using the EM Algorithm with the algorithm stopping at the eleventh iteration. With these lambdas, we were able to get 21.996 on training perplexity while 1.028 on perplexity on unseen / test data. We were also able to generate two sentences as shown above. \n",
    "\n",
    "\n",
    "10 random JSON files used for testing:\n",
    "\n",
    "d6cd8944-d555-455f-80ad-b3c7b4f91378.json.JSON <br>\n",
    "c7f66770-5fb9-4e86-8d22-51a0aa2bbee9.json.JSON <br>\n",
    "d7a567cf-31a1-4b6f-b1bf-c7eb7f0acea4.json.JSON <br>\n",
    "1fb0ce33-bbdf-4fcb-b110-5494cf84fc1f.json.JSON <br>\n",
    "57874f05-16b2-4951-bd04-9c8273c8f465.json.JSON <br>\n",
    "10baef64-43cf-477e-9b1b-8e0f4b568ca4.json.JSON <br>\n",
    "b345ff09-64bf-4473-935f-fef4f9da6f23.json.JSON <br>\n",
    "ebf6d539-b774-4828-9500-c93302fd4e67.json.JSON <br>\n",
    "47c625f2-e722-41b3-8373-9a07224f99a0.json.JSON <br>\n",
    "c29968c7-7f30-400a-8725-0f62cbcb8061.json.JSON\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
